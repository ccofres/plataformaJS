<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link
      href="https://fonts.googleapis.com/css2?family=Oswald&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=Orbitron&display=swap"
      rel="stylesheet"
    />
    <link rel="preconnect" href="https://fonts.gstatic.com" />
    <link
      href="https://fonts.googleapis.com/css2?family=Merriweather&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="../../css/styles.css" />
    <link rel="stylesheet" href="../../css/art-styles.css" />
    <link rel="stylesheet" href="../../css/style-sidebar.css" />
    <title>Breve historia AI-DL</title>
  </head>
  <body>
    <!-- THE HEADER - SECOND HEADER-->
    <div id="the-header">
      <header>
        <div class="second-header">
          <h1><a href="../../index.html"> Plataforma JS </a></h1>
        </div>
      </header>
    </div>

    <!-- NAV -  BARRA NAVEGACION-->
    <div id="the-nav">
      <nav>
        <ul id="menu">
          <!--           <li><a href="../../../index.html">Home</a></li> -->
          <li class="dropdown">
            <a href="javascript:void(0)" class="dropbtn">Let's play!</a>
            <div class="dropdown-content">
              <a href="../modelos.html">Modelos</a>
              <a href="../editorjs.html">Editor JS</a>
            </div>
          </li>

          <li class="dropdown">
            <a href="javascript:void(0)" class="dropbtn">TensorFlow.js</a>
            <div class="dropdown-content">
              <a href="../tensorflowjs/how-works.html"
                >¿Cómo se trabaja en tfjs?</a
              >
              <a href="../tensorflowjs/recursos.html">Otros recursos</a>
            </div>
          </li>

          <li>
            <a style="cursor: pointer" onclick="openNav()">&#9776; Ejemplos</a>
          </li>

          <li class="dropdown">
            <a href="javascript:void(0)" class="dropbtn">Recursos externos</a>
            <div class="dropdown-content">
              <a href="https://js.tensorflow.org/api/latest/" target="_blank"
                >Referencia API</a
              >

              <a
                href="https://github.com/tensorflow/tfjs-models"
                target="_blank"
                >Modelos tfjs</a
              >
              <a
                href="https://github.com/tensorflow/tfjs-examples/"
                target="_blank"
                >Más ejemplos tfjs</a
              >
              <a
                href="https://blog.tensorflow.org/search?label=TensorFlow.js&max-results=10"
                target="_blank"
                >Blog TensorFlow.js</a
              >
              <a href="https://playground.tensorflow.org/" target="_blank"
                >Tensorflow Playground</a
              >
              <a href="https://poloclub.github.io/ganlab/" target="_blank"
                >GAN Lab</a
              >
            </div>
          </li>
        </ul>
      </nav>
    </div>

    <div id="contenido-html">
      <div class="art-html">
        <div class="stackedit__html">
          <h1 id="breve-historia-de-la-ia-y-el-aprendizaje-profundo">
            Breve historia de la IA y el Aprendizaje Profundo
          </h1>
          <p>
            La Inteligencia Artificial, y más específicamente el Aprendizaje
            Profundo, está basado en ideas de los años 60, 80 y 90. A pesar de
            que se puede pensar que son descubrimientos recientes, es un campo
            que ha tenido una lenta y gradual evolución a través de varias
            décadas. A continuación se presenta un resumen no exhaustivo de
            parte de los grandes hitos en su desarrollo:
          </p>
          <ul>
            <li>
              <strong>1943:</strong> Warren S. McCulloch y Walter Pitts llevan
              el modelo conocido de la neuronas a la matemática, mostrando con
              esto el primero modelo matemático de la neurona biológica.
            </li>
            <li>
              <strong>1957:</strong> Frank F. Rosenblatt basandose en el modelo
              matemático propuesto por Pitts y McCulloch, propone la idea del
              <em>Perceptron</em>, una red neuronal de una sola capa capaz de
              realizar clasificación binaria. Un par de años más tarde (1962)
              propone también la idea de una red de múltiples capas.
            </li>
            <li>
              <strong>1960:</strong> Henry J. Kelley muestra por primera vez un
              modelo de propagación hacia atrás o Backpropagation en el contexto
              de la Teoría de Control.
            </li>
            <li>
              <strong>1962:</strong> Stuart Dreyfus demuestra un modelo de
              Backpropagation que funciona utilizando simples derivadas y la
              regla de la cadena.
            </li>
            <li>
              <strong>1965:</strong> Alexey Grigoryevich Ivakhnenko y Valentin
              Grigorevich Lapa crean el primer algorítmo de aprendizaje para el
              entrenamiento de perceptrones multicapa, utilizando entrenamiento
              GMDH (Group Method of Data Handling o Método de agrupamiento para
              el manejo de datos) y funciones de activación polinomial. Son
              considerador por esto como los padres del <em>Deep Learning</em>.
            </li>
            <li>
              <strong>1970:</strong> Seppo Linnainmaa presenta en su tesis de
              Master y en un artículo posterior el primer código de
              diferenciación automática y Backpropagation.
            </li>
            <li>
              <strong>1980:</strong> Kunihiko Fukushima inventa las
              Convolutional Neural Network (CNN) al proponer su arquitectura
              <em>Neocognitron</em> capaz de reconocer patrones visuales.
            </li>
            <li>
              <strong>1982:</strong> John Hopfield crea la primera arquitectura
              reconocida como una Recurrent Neural Network (RNN). Esta misma
              sería conocida más tarde como <em>Hopfield Network</em>, siendo
              fundamental para los modelos posteriores de RNN.
            </li>
            <li>
              <strong>1985:</strong> David H.Ackley, Geoffrey E.Hinton y
              Terrence J.Sejnowski crean la Boltzmann Machine, una especie de
              RNN estocástica sin capa de salida.
            </li>
            <li>
              <strong>1986:</strong> David E. Rumelhart, Geoffrey E. Hinton y
              Ronald J. Williams muestran la primera implementación exitosa del
              algorítmo de Backpropagation en el entrenamiento de las redes
              neuronales.
            </li>
            <li>
              <strong>1989:</strong> Yann LeCun muestra la primera
              implementación del algorítmo de Backpropagation en el
              entrenamiento de Convolutional Neural Network.
            </li>
            <li>
              <strong>1997:</strong> Sepp Hochreiter and Jürgen Schmidhuber
              proponen la arquitectura Long Short-Term Memory (LSTM), una
              arquitectura de red tipo RNN con conexiones del tipo
              <em>feedback</em> que le permiten procesar secuencias de datos.
            </li>
            <li>
              <strong>2006:</strong> Geoffrey E. Hinton, Simon Osindero y Yee
              Whye Teh crean la Deep Belief Network y mejoran el proceso de
              entrenamiento haciendolo más eficiente. Se comienza a utilizar el
              término <em>Deep Learning</em> con la connotación que hoy se
              conoce.
            </li>
            <li>
              <strong>2009:</strong> Fei-Fei Li lanza ImageNet. Base de datos de
              más de 14 millones de imágenes y más de 20000 categorías
              debidamente etiquetadas.
            </li>
            <li>
              <strong>2012:</strong> Alex Krizhevsky presenta AlexNet en NIPS,
              un modelo o arquitectura que implementa Convolutional Neural
              Networks mediante GPU, alcanzando una capacidad en la
              clasificación de imágenes de ImageNet del estado del arte.
            </li>
            <li>
              <strong>2014:</strong> Ian Goodfellow propone las Generative
              Adversarial Network, conocidas como GAN. Con ello se abre la
              puerta a la aplicación de generación de imagénes y video debido a
              su gran capacidad de sintetizar datos.
            </li>
          </ul>
          <br />
          <div>
            Para un cuadro más exhaustivo de la historia de la IA y el Deep
            Learning, se sugiere revisar el árticulo
            <a
              href="https://arxiv.org/abs/1404.7828"
              target="_blank"
              rel="noopener noreferrer"
            >
              “Deep Learning in Neural Networks: An Overview” por Juergen
              Schmidhuber</a
            >, en el que intenta hacer una recopilación de cada una las personas
            que han contribuido al desarrollo de este campo.
          </div>
        </div>
      </div>
    </div>

    <!-- FOOTER -  PIE DE PAGINA-->
    <div id="the-footer">
      <footer>
        <p>Legal disclaimer, copyright, etc.</p>
        <ul>
          <li>
            <a href="#"
              ><img src="../../img/icon1.png" alt="Social Media 1."
            /></a>
          </li>
          <li>
            <a href="#"
              ><img src="../../img/icon2.png" alt="Social Media 2."
            /></a>
          </li>
        </ul>
      </footer>
    </div>
    <!-- SIDENAV -->
    <nav id="mySidenav" class="sidenav"></nav>
    <!-- FIN SideNav-->
    <script src="../tensorflowjs/ui-sidebar.js"></script>
  </body>
</html>
