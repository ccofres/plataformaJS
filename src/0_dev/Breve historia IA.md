### Breve historia de la IA y el Aprendizaje Profundo

La Inteligencia Artificial, y más específicamente el Aprendizaje Profundo, está basado en ideas de los años 60, 80 y 90. A pesar de que se puede pensar que son descubrimientos recientes, es un campo que ha tenido una lenta y gradual evolución a través de varias décadas. A continuación se presenta un resumen no exhaustivo de parte de los grandes hitos en su desarrollo:

- **1943:** Warren S. McCulloch y Walter Pitts llevan el modelo conocido de la neuronas a la matemática, mostrando con esto el primero modelo matemático de la neurona biológica.
- **1957:** Frank F. Rosenblatt basandose en el modelo matemático propuesto por Pitts y McCulloch, propone la idea del *Perceptron*, una red neuronal de una sola capa capaz de realizar clasificación binaria. Un par de años más tarde (1962) propone también la idea de una red de múltiples capas.
- **1960:** Henry J. Kelley muestra por primera vez un modelo de propagación hacia atrás o Backpropagation en el contexto de la Teoría de Control.
- **1962:** Stuart Dreyfus demuestra un modelo de Backpropagation que funciona utilizando simples derivadas y la regla de la cadena.
- **1965:** Alexey Grigoryevich Ivakhnenko y Valentin Grigorevich Lapa crean el primer algorítmo de aprendizaje para el entrenamiento de perceptrones multicapa, utilizando entrenamiento GMDH (Group Method of Data Handling o Método de agrupamiento para el manejo de datos) y funciones de activación polinomial. Son considerador por esto como los padres del *Deep Learning*.
- **1970:** Seppo Linnainmaa presenta en su tesis de Master y en un  artículo posterior el primer código de diferenciación automática y Backpropagation.
- **1980:** Kunihiko Fukushima inventa las Convolutional Neural Network (CNN) al proponer su arquitectura *Neocognitron* capaz de reconocer patrones visuales.
- **1982:** John Hopfield crea la primera arquitectura reconocida como una Recurrent Neural Network (RNN). Esta misma sería conocida más tarde como *Hopfield Network*, siendo fundamental para los modelos posteriores de RNN.
- **1985:** David H.Ackley, Geoffrey E.Hinton y Terrence J.Sejnowski crean la Boltzmann Machine, una especie de RNN estocástica sin capa de salida.
- **1986:** David E. Rumelhart, Geoffrey E. Hinton y Ronald J. Williams muestran la primera implementación exitosa del algorítmo de Backpropagation en el entrenamiento de las redes neuronales.
- **1989:** Yann LeCun muestra la primera implementación del algorítmo de Backpropagation en el entrenamiento de Convolutional Neural Network.
- **1997:** Sepp Hochreiter and Jürgen Schmidhuber proponen la arquitectura Long Short-Term Memory (LSTM), una arquitectura de red tipo RNN con conexiones del tipo *feedback* que le permiten procesar secuencias de datos.
- **2006:** Geoffrey E. Hinton, Simon Osindero y Yee Whye Teh crean la Deep Belief Network y mejoran el proceso de entrenamiento haciendolo más eficiente. Se comienza a utilizar el término *Deep Learning* con la connotación que hoy se conoce.
- **2009:** Fei-Fei Li lanza ImageNet. Base de datos de más de 14 millones de imágenes y más de 20000 categorías debidamente etiquetadas.
- **2012:** Alex Krizhevsky presenta AlexNet en NIPS, un modelo o arquitectura que implementa Convolutional Neural Networks mediante GPU, alcanzando una capacidad en la clasificación de imágenes de ImageNet del estado del arte.
- **2014:** Ian Goodfellow propone las Generative Adversarial Network, conocidas como GAN. Con ello se abre la puerta a la aplicación de generación de imagénes y video debido a su gran capacidad de sintetizar datos.

 
Para un cuadro más exhaustivo de la historia de la IA y el Deep Learning, se sugiere revisar el árticulo "Deep Learning in Neural Networks: An Overview" por Juergen Schmidhuber, en el que intenta hacer una recopilación de cada una las personas que han contribuido al desarrollo del Deep Learning.
