<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Breve historia IA</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h3 id="breve-historia-de-la-ia-y-el-aprendizaje-profundo">Breve historia de la IA y el Aprendizaje Profundo</h3>
<p>La Inteligencia Artificial, y más específicamente el Aprendizaje Profundo, está basado en ideas de los años 60, 80 y 90. A pesar de que se puede pensar que son descubrimientos recientes, es un campo que ha tenido una lenta y gradual evolución a través de varias décadas. A continuación se presenta un resumen no exhaustivo de parte de los grandes hitos en su desarrollo:</p>
<ul>
<li><strong>1943:</strong> Warren S. McCulloch y Walter Pitts llevan el modelo conocido de la neuronas a la matemática, mostrando con esto el primero modelo matemático de la neurona biológica.</li>
<li><strong>1957:</strong> Frank F. Rosenblatt basandose en el modelo matemático propuesto por Pitts y McCulloch, propone la idea del <em>Perceptron</em>, una red neuronal de una sola capa capaz de realizar clasificación binaria. Un par de años más tarde (1962) propone también la idea de una red de múltiples capas.</li>
<li><strong>1960:</strong> Henry J. Kelley muestra por primera vez un modelo de propagación hacia atrás o Backpropagation en el contexto de la Teoría de Control.</li>
<li><strong>1962:</strong> Stuart Dreyfus demuestra un modelo de Backpropagation que funciona utilizando simples derivadas y la regla de la cadena.</li>
<li><strong>1965:</strong> Alexey Grigoryevich Ivakhnenko y Valentin Grigorevich Lapa crean el primer algorítmo de aprendizaje para el entrenamiento de perceptrones multicapa, utilizando entrenamiento GMDH (Group Method of Data Handling o Método de agrupamiento para el manejo de datos) y funciones de activación polinomial. Son considerador por esto como los padres del <em>Deep Learning</em>.</li>
<li><strong>1970:</strong> Seppo Linnainmaa presenta en su tesis de Master y en un  artículo posterior el primer código de diferenciación automática y Backpropagation.</li>
<li><strong>1980:</strong> Kunihiko Fukushima inventa las Convolutional Neural Network (CNN) al proponer su arquitectura <em>Neocognitron</em> capaz de reconocer patrones visuales.</li>
<li><strong>1982:</strong> John Hopfield crea la primera arquitectura reconocida como una Recurrent Neural Network (RNN). Esta misma sería conocida más tarde como <em>Hopfield Network</em>, siendo fundamental para los modelos posteriores de RNN.</li>
<li><strong>1985:</strong> David H.Ackley, Geoffrey E.Hinton y Terrence J.Sejnowski crean la Boltzmann Machine, una especie de RNN estocástica sin capa de salida.</li>
<li><strong>1986:</strong> David E. Rumelhart, Geoffrey E. Hinton y Ronald J. Williams muestran la primera implementación exitosa del algorítmo de Backpropagation en el entrenamiento de las redes neuronales.</li>
<li><strong>1989:</strong> Yann LeCun muestra la primera implementación del algorítmo de Backpropagation en el entrenamiento de Convolutional Neural Network.</li>
<li><strong>1997:</strong> Sepp Hochreiter and Jürgen Schmidhuber proponen la arquitectura Long Short-Term Memory (LSTM), una arquitectura de red tipo RNN con conexiones del tipo <em>feedback</em> que le permiten procesar secuencias de datos.</li>
<li><strong>2006:</strong> Geoffrey E. Hinton, Simon Osindero y Yee Whye Teh crean la Deep Belief Network y mejoran el proceso de entrenamiento haciendolo más eficiente. Se comienza a utilizar el término <em>Deep Learning</em> con la connotación que hoy se conoce.</li>
<li><strong>2009:</strong> Fei-Fei Li lanza ImageNet. Base de datos de más de 14 millones de imágenes y más de 20000 categorías debidamente etiquetadas.</li>
<li><strong>2012:</strong> Alex Krizhevsky presenta AlexNet en NIPS, un modelo o arquitectura que implementa Convolutional Neural Networks mediante GPU, alcanzando una capacidad en la clasificación de imágenes de ImageNet del estado del arte.</li>
<li><strong>2014:</strong> Ian Goodfellow propone las Generative Adversarial Network, conocidas como GAN. Con ello se abre la puerta a la aplicación de generación de imagénes y video debido a su gran capacidad de sintetizar datos.</li>
</ul>
<p>Para un cuadro más exhaustivo de la historia de la IA y el Deep Learning, se sugiere revisar el árticulo “Deep Learning in Neural Networks: An Overview” por Juergen Schmidhuber, en el que intenta hacer una recopilación de cada una las personas que han contribuido al desarrollo del Deep Learning.</p>
</div>
</body>

</html>
